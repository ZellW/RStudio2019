---
title: "Reuters -- Single-label, Multi-class Classification with Text"
subtitle: "Scenario 3: GloVe, pre-trained word embeddings"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)

# Initialize package
library(keras)
library(tidyverse)
```

# {.tabset .tabset-fade .tabset-pills}

## Learning Goals & Functions

### Learning Goals

Text analysis using pre-trained word embeddings, o.e. from GloVe.

### Functions in this session:

Here we'll see some basic ways of working with text, using one-hot encoding.

| Function                                   | Description                                                         |
|:-------------------------------------------|:--------------------------------------------------------------------|
| `text_tokenizer()`, `fit_text_tokenizer()` | Produce tokens environment from text.                               |
| `texts_to_sequences()`                     | Produce vectorized values from text.                                |
| `get_layer()`                              | Access a specific layer in a model                                  |  
| `set_weights()`                            | Manually set the weights, from e.g. a pre-trained embedding matrix. |

## Obtain data

In the exercises you'll read text from files, here we'll just use the build-in examples.

```{r data, warning = FALSE}

c(c(train_data, train_labels), c(test_data, test_labels)) %<-% dataset_reuters(num_words = 10000)

# Map values back onto the lexicon which is stored as a named list. Each item in the list is an integer vector of length one. This number corresponds to the position in the word count list and the name of the vector is the actual word. 

dataset_reuters_word_index() %>% 
  unlist() %>%                      # produce a vector
  sort() %>%                        # put them in order 
  names() -> word_index             # take the ordered names

# Here, we'll get the actual text of the newswire from the integer data. If you read text direclty from a file you don't need to worry aobut this. 
train_data %>%
  map(~ word_index[.x]) %>%
  map(~ paste(.x, collapse = " ")) %>% 
  unlist() -> texts
```

## Tokenize the data

Let's vectorize the texts we collected, and prepare a training and validation split. We will merely be using the concepts we introduced earlier in this section.

Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 samples. So we will be learning to classify movie reviews after looking at just 200 examples...

```{r}
# We will only consider the top 10,000 words in the dataset
max_words <- 10000

tokenizer <- _______(num_words = max_words) %>% 
  _______(texts)

typeof(tokenizer)
```

We have now tokenized the text, the first step in our process. Let's explore:

```{r}
word_index <- tokenizer$word_index
head(word_index)
```

Number of unique tokens:

```{r}
length(tokenizer$word_index)
```

```{r}
index_word <- names(sort(unlist(tokenizer$word_index)))

index_word[1:10]
```

```{r}
sequences <- _______(tokenizer, texts)
```

`sequences` contains the vectorized values as a list.

```{r}
# The vectorized first instance:
sequences[[1]]
```

What the text has become:

```{r} 
paste(index_word[sequences[[1]]] ,collapse=" ")
```

From our original text:

```{r}
texts[[1]]
```

Like before, we'll limit ourselves to the first 500 words.

```{r}

maxlen <- 100 # We will cut reviews after 100 words

data <- pad_sequences(sequences, maxlen = maxlen)
```

```{r}
data[1,]
```

```{r}
paste(unlist(index_word)[data[1,]] ,collapse=" ")
```

Shape of data tensor: `r dim(data)`, Shape of label tensor: `r dim(labels)`.

And here, instead of letting the fit function produce a validation set split, we'll do that manually.

But first, we'll need to shuffle the data, since we started from an ordered data set (all negative, followed by all positive).

```{r}
indices <- sample(1:nrow(data))

# training_samples <- nrow(data)/2       # We will be training on 4491 samples
# validation_samples <- nrow(data)/2   # We will be validating on 4491 samples

n <- 7500
training_samples <- n   
validation_samples <- 8982-n   


training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

x_train <- data[training_indices,]

# Prepare the labels
labels <- as.array(train_labels)
y_train <- labels[training_indices]


x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

## GloVe word embeddings

The GloVE pre-trained word embeddings are derived from the 2014 English Wikipedia. It can be downloaded at [here](https://nlp.stanford.edu/projects/glove/). The file to look for is `glove.6B.zip` (822MB). In it you'll find  100-dimensional embedding vectors for 400,000 words (or non-word tokens).

Download and unzip the file (it will just a be a `txt` file) and place it in the _pre-trained_ folder.

Here are the commands if you want to do this via the terminal:  

```
mkdir pretrained
cd pretrained/
mkdir glove.6B
cd glove.6B
wget "http://nlp.stanford.edu/data/glove.6B.zip"
unzip glove.6B.zip
rm glove.6B.zip 
```

### Pre-process the embeddings

We'll parse the `txt` file to build an index mapping words (as strings) to their vector representation (as number 
vectors).

```{r}

glove_dir <- "/../../usr/share/class/glove.6B"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

lines[1] # Word embeddings in 100 dimensions.
```

The word embeddings are contained in strings that contain the word and then 100 numbers separated by a space. We need to get an embedding matrix that we can use 

Here we'll use a nice trick to work with the embeddings by making them into an environment instead of a list.

```{r}

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())

for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

# length(embeddings_index)
# 400000
```

i.e. the 100-dimensional embedding vectors for 400,000 words.

```{r}
_______[["said"]]
```

Now we're ready to build an embedding matrix that we can load into an embedding layer.

It must be a matrix of shape `(max_words, embedding_dim)`, where each entry _i_ contains the `embedding_dim`-dimensional vector for the word of index _i_ in the reference word index (built during tokenization). Note that index 1 isn't supposed to stand for any word or token -- it's a placeholder.

```{r}
embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

## Define a model

We will be using the same model architecture as before:

```{r}
model <- _______() %>% 
  
  # embeddings layer
  _______(input_dim = _______, # 10000 tokens
                  output_dim = embedding_dim, # dimensions in embedding layer
                  input_length = maxlen) %>%  # length of each review set by maxlen
  
  # flatten output
  _______() %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 46, activation = "softmax")

summary(model)
```

### Load the GloVe embeddings in the model

The embedding layer has a single weight matrix: a 2D float matrix where each entry _i_ is the word vector meant to be associated with index _i_. Simple enough. Load the GloVe matrix you prepared into the embedding layer, the first layer in the model.



```{r}

# get the layer, set the weights and then freeze them.

_______(model, index = 1) %>% 
  _______(list(embedding_matrix)) %>% 
  _______()
```

Additionally, you'll freeze the weights of the embedding layer, following the same rationale you're already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like your embedding layer) and parts are randomly initialized (like your classifier), the pre-trained parts shouldn't be updated during training, to avoid forgetting  what they already know. The large gradient updates triggered by the randomly initialized layers would be disruptive to the already-learned features.

### Train and evaluate

Let's compile our model and train it:

```{r, echo=TRUE, results='hide'}
model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")
```

Plot the performance over time:

```{r}
plot(history)
```

The model quickly starts over-fitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for the same reason, but seems to reach the 50s.

We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings when lots of data is available. Let's try it:

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```

Validation accuracy stalls in the low 70s. In this case, pre-trained word embeddings doesn't outperform jointly-learned embeddings. If we had a small number of training samples, this would not be the case, pre-trained word embeddings would out-perform the jointly-learned embeddings.

## Evaluation

Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:

```{r}
test_data %>%
  # unlist %>% 
  map(~ word_index[.x]) %>%
  map(~ paste(.x, collapse = " ")) %>% 
  unlist() -> texts
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(test_labels)

```

And let's load and evaluate the first model:

```{r}
model %>% 
  load_model_weights_hdf5("pre_trained_glove_model.h5") %>% 
  evaluate(x_test, y_test, verbose = 0)
```

We get an appalling test accuracy :( Working with just a handful of training samples for so many classes is difficult, plus in some classes we only have a few instances!


```{r}
model %>% 
  evaluate(x_test, y_test, verbose = 0)
```
