---
title: "Reuters -- Single-label, Multi-class Classification with Text"
subtitle: "Scenario 5: Long short-term memory (LSTM)"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)

# Initialize package
library(keras)
library(tidyverse)
```

# {.tabset .tabset-fade .tabset-pills}

## Learning Goals & Functions

### Learning Goals

Implement an LSTM model for text (i.e. sequence) analysis.

### Functions in this session:

| Function       | Description                  |
|:---------------|:-----------------------------|
| `layer_lstm()` | Add an LSTM layer to a model. |

## Obtain data

Let's take a look at an RNN on the Reuters dataset. First with only 20 words.

```{r}

library(keras)

# Number of most common words to consider as features
max_features <- 10000

# Loads the data as lists of integers
c(c(input_train_original, train_targets), c(input_test_original, test_targets)) %<-% dataset_reuters(num_words = max_features)

# Cut off the text after 20 words (i.e. among the max_features most common words)
maxlen <- 100

# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)
input_train <- pad_sequences(input_train_original, maxlen = maxlen)
input_test <- pad_sequences(input_test_original, maxlen = maxlen)
```

## LSTM in Keras

Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the Reuters dataset. Here's the network,similar to the one with `layer_simple_rnn()` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every  other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always "just work" without you having to spend time tuning parameters by hand.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop", 
  loss = "sparse_categorical_crossentropy", 
  metrics = "accuracy"
)

history <- model %>% fit(
  input_train, train_targets,
  epochs = 20,
  batch_size = 128,
  validation_split = 0.2
)
```

```{r}
plot(history)
```

## Evaluation

Let's evaluate the model on the test data. 

```{r}
model %>% 
  evaluate(input_test, test_targets, verbose = 0)
```

We can also load specific weights:

```{r eval = FALSE }
# model %>% 
#   load_model_weights_hdf5("pre_trained_glove_model.h5") %>% 
#   evaluate(input_test, test_targets, verbose = 0)
```