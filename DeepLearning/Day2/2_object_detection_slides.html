<!DOCTYPE html>
<html>
  <head>
    <title>Object detection</title>
    <meta charset="utf-8">
    <meta name="author" content="Sigrid Keydana" />
    <link rel="stylesheet" href="theme/rstudio.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Object detection
### Sigrid Keydana
### rstudio::conf 2019

---







# Learning objectives

- Perform classification and localization on a single object 

- Important concepts in multiple-object detection

- Understand how to code a _very_ basic version of SSD (__Single-Shot Multibox Detector__)

- Understand the options to improve on this basic detector

---
class: inverse, middle, center

# Road to object detection

---
# Single-object classification and localization

- Mix of recap (if you've participated in yesterday's workshop) and new topics

- Partly demo, partly exercise

- We'll also look at the dataset and preprocessing required for this session


---
# PASCAL Visual Object Classes (VOC) challenges and datasets

- Challenges (2005-2012) included
 - classification (presence/absence for each object class)
 - object detection (same as above, plus localization)
 - class segmentation
 - "person layout"
 
- We'll use the training set from the [2007 challenge](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html)

- Number of object classes: 20

- Number of training images: 2501

- We focus on concepts and the how-to, not accuracy

---
# Object detection examples

![](2_object_detection/images/birds_scaled.png)

![](2_object_detection/images/bicycles_scaled.png)


---
# How do you learn bounding boxes?

- Can be framed as __regression problem__

 - often trained with mean absolute error

- Predict pixel coordinates of box corners (_x_left_, _y_top_, _x_right_, _y_bottom_)

- Relevant metric is __Intersection over Union__ (__IOU__), also known as Jaccard index&lt;sup&gt;1&lt;/sup&gt;


.footnote[[1] Image source: Wikipedia.]

![](2_object_detection/images/iou.png)

---
# Demo/exercise: Single-object classification and localization

- Notebook: [2_object_detection/1_classification_localization.Rmd](2_object_detection/1_classification_localization.Rmd)

- Quiz: [2_object_detection/object_detection_quizzes.Rmd](2_object_detection/object_detection_quizzes.Rmd)


---
class: inverse, middle, center

# Introduction to multiple-object detection


---
# Why can't we just have more bounding boxes?


Each bounding box detector will try to detect all objects:&lt;sup&gt;1&lt;/sup&gt;

![](2_object_detection/images/2boxes.jpg)

We need to either:

- zoom in on single objects in some way, or
- have detectors __specialize__ in what to detect

.footnote[[1] Image source: http://machinethink.net/blog/object-detection/]

---
# Object detection main approaches / paradigms

- Sliding windows approaches
 - Train network, run sequentially on image patches 
 - May actually run sliding windows synchronously (see _Overfeat_ below)

- Region proposal approaches (2-step)
 - Step 1: Some algorithm proposes interesting regions
 - Step 2: Another algorithm (a convnet) classifies the regions and refines localization

- Single-shot detectors (YOLO, SSD)
 - Perform detection, classification and localization in one step

---
# Sliding windows done synchronously



.footnote[[1] cf. Sermanet, P, D. Eigen, X. Zhang, et al. (2013). OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. ]


![](2_object_detection/images/overfeat.png)

---
# Region proposal approaches

- R-CNN&lt;sup&gt;1&lt;/sup&gt;: Uses non-DL algorithm to select interesting regions, then applies CNN to all identified regions sequentially

- Fast R-CNN&lt;sup&gt;2&lt;/sup&gt;: Uses non-DL algorithm to select interesting regions, then classifies all regions in one pass

- Faster R-CNN&lt;sup&gt;3&lt;/sup&gt;: Uses a convnet for region proposal (_Region proposal network_), then classifies all regions in one pass



&lt;span class="footnote"&gt;
[1] cf. Girshick, R. B. (2015). Fast R-CNN.&lt;br /&gt;
[2] cf. Girshick, R. B, J. Donahue, T. Darrell, et al. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation.&lt;br /&gt;
[3] cf. Ren, S, K. He, R. B. Girshick, et al. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
&lt;/span&gt;




---
# Single-shot detectors

Make detectors __specialize__ using __anchor boxes__&lt;sup&gt;1&lt;/sup&gt;

![](2_object_detection/images/ssd_1.png)



.footnote[[1] illustration from: Liu, W, D. Anguelov, D. Erhan, et al. (2015). SSD: Single Shot MultiBox Detector.]


---
# Why do anchor boxes help?

### Concept 1: Grid

By laying a grid over the image, we _map detectors to image regions_.

### Concept 2: Different aspect ratios and scales

Objects of different shapes are localized more easily because we lean on different customized _priors_.

&lt;br /&gt;

&lt;img src="2_object_detection/images/ssd_1.png" width = "50%"&gt;

---
# SSD architecture

&lt;br /&gt;

How do we detect objects that are of completely different sizes than a grid cell (e.g., span the whole image)?

SSD adds anchor boxes at different resolutions:

&lt;img src="2_object_detection/images/ssd_arch.png" width = "120%"&gt;

---
# SSD vs. YOLO

- Early YOLO versions (v1&lt;sup&gt;1&lt;/sup&gt;/v2&lt;sup&gt;2&lt;/sup&gt;) had dense layers before the final output, but current YOLO&lt;sup&gt;3&lt;/sup&gt; is fully convolutional just like SSD

- In addition to _number of classes_ class scores and 4 bounding box coordinates __per detector__, YOLO also has a _confidence score_

- YOLO determines anchor boxes for each dataset individually (using k-means clustering on the actual bounding boxes), while SSD uses _default boxes_

- Details of bounding box computations differ



&lt;span class="footnote"&gt;
[1] cf. Redmon, J, S. K. Divvala, R. B. Girshick, et al. (2015). You Only Look Once: Unified, Real-Time Object Detection. &lt;br /&gt;
[2] cf. Redmon, J. and A. Farhadi (2016). YOLO9000: Better, Faster, Stronger. &lt;br /&gt;
[3] cf. Redmon, J. and A. Farhadi (2018). YOLOv3: An Incremental Improvement.
&lt;/span&gt;

---
# Why fully convolutional?

As long as for every detector, the part of the image it's responsible for is visible (= is in its receptive field), we don't need a fully connected layer at the end&lt;sup&gt;1&lt;/sup&gt;

&lt;br /&gt;

&lt;img src="2_object_detection/images/receptive_field.png" width = "50%"/&gt;



.footnote[[1] Image from: Goodfellow, I, Y. Bengio and A. Courville (2016). Deep Learning.]

---
class: inverse, middle, center

# Coding a (very!) basic single-shot detector


---
# Basic SSD: Code

- To show the basic approach, we will
 - restrict ourselves to a 4x4 grid of image cells
 - have one anchor box per cell (thus, 16 anchor boxes)
 - don't aggregate detections from different resolutions 

- Notebook: [2_object_detection/2_object_detection_ssd.Rmd](2_object_detection/2_object_detection_ssd.Rmd)


---
# Basic SSD: Ways for improvement

- Use focal loss

- Use anchor boxes of different aspect ratios

- Perform detection at various resolutions

- (not object detection specific:) Use data augmentation


---
# Focal Loss&lt;sup&gt;1&lt;/sup&gt;



&lt;img src="2_object_detection/images/focal.png" width = "70%"/&gt;



.footnote[[1] Image from: Lin, T, P. Goyal, R. B. Girshick, et al. (2017). Focal Loss for Dense Object Detection.]


---
# Focal loss, and more anchors

&lt;br /&gt;

Implementation stubs:

- Focal loss: [2_object_detection/3_object_detection_ssd_focal.Rmd](2_object_detection/3_object_detection_ssd_focal.Rmd)


- More anchors: [2_object_detection/4_object_detection_ssd_moreanchors.Rmd](2_object_detection/4_object_detection_ssd_moreanchors.Rmd)

---
# Wrapup / feedback

---
# References

Girshick, R. B. (2015). "Fast R-CNN". In: _CoRR_ abs/1504.08083.
eprint: 1504.08083. URL:
[http://arxiv.org/abs/1504.08083](http://arxiv.org/abs/1504.08083).

Girshick, R. B, J. Donahue, T. Darrell, et al. (2013). "Rich
feature hierarchies for accurate object detection and semantic
segmentation". In: _CoRR_ abs/1311.2524. eprint: 1311.2524. URL:
[http://arxiv.org/abs/1311.2524](http://arxiv.org/abs/1311.2524).

Goodfellow, I, Y. Bengio and A. Courville (2016). _Deep Learning_.
&lt;URL: http://www.deeplearningbook.org&gt;. MIT Press.

Lin, T, P. Goyal, R. B. Girshick, et al. (2017). "Focal Loss for
Dense Object Detection". In: _CoRR_ abs/1708.02002. eprint:
1708.02002. URL:
[http://arxiv.org/abs/1708.02002](http://arxiv.org/abs/1708.02002).

Liu, W, D. Anguelov, D. Erhan, et al. (2015). "SSD: Single Shot
MultiBox Detector". In: _CoRR_ abs/1512.02325. eprint: 1512.02325.
URL:
[http://arxiv.org/abs/1512.02325](http://arxiv.org/abs/1512.02325).

---
# References (cont.)

Redmon, J, S. K. Divvala, R. B. Girshick, et al. (2015). "You Only
Look Once: Unified, Real-Time Object Detection". In: _CoRR_
abs/1506.02640. eprint: 1506.02640. URL:
[http://arxiv.org/abs/1506.02640](http://arxiv.org/abs/1506.02640).

Redmon, J. and A. Farhadi (2016). "YOLO9000: Better, Faster,
Stronger". In: _CoRR_ abs/1612.08242. eprint: 1612.08242. URL:
[http://arxiv.org/abs/1612.08242](http://arxiv.org/abs/1612.08242).

Redmon, J. and A. Farhadi (2018). "YOLOv3: An Incremental
Improvement". In: _CoRR_ abs/1804.02767. eprint: 1804.02767. URL:
[http://arxiv.org/abs/1804.02767](http://arxiv.org/abs/1804.02767).

Ren, S, K. He, R. B. Girshick, et al. (2015). "Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks".
In: _CoRR_ abs/1506.01497. eprint: 1506.01497. URL:
[http://arxiv.org/abs/1506.01497](http://arxiv.org/abs/1506.01497).

Sermanet, P, D. Eigen, X. Zhang, et al. (2013). "OverFeat:
Integrated Recognition, Localization and Detection using
Convolutional Networks". In: _CoRR_ abs/1312.6229. eprint:
1312.6229. URL:
[http://arxiv.org/abs/1312.6229](http://arxiv.org/abs/1312.6229).
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
