---
title: "Reuters -- Single-label, Multi-class Classification with Text"
subtitle: "Scenario 4: Embeddings with RNNs"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# Initialize package
library(keras)
library(tidyverse)
```

# {.tabset .tabset-fade .tabset-pills}

## Learning Goals & Functions

### Learning Goals

Implement an RNNs with the Reuters data set.

### Functions in this session:

As previously seen:

| Function             | Description                  |
|:---------------------|:-----------------------------|
| `layer_simple_rnn()` | Add an RNN layer to a model. |

## Reuters in an RNN

Let's take a look at an RNN on the Reuters dataset. First with only 20 words.

```{r}

library(keras)

# Number of most common words to consider as features
max_features <- 10000

# Loads the data as lists of integers
c(c(input_train_original, train_targets), c(input_test_original, test_targets)) %<-% dataset_reuters(num_words = max_features)

# Cut off the text after 20 words (i.e. among the max_features most common words)
maxlen <- 20

# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)
input_train <- pad_sequences(input_train_original, maxlen = maxlen)
input_test <- pad_sequences(input_test_original, maxlen = maxlen)
```

Let's train a simple recurrent network using a `layer_embedding()` and `layer_simple_rnn()`.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

historyRNN_small <- model %>% fit(
  input_train, train_targets,
  epochs = 20,
  batch_size = 128,
  validation_split = 0.15
)
```


```{r}
plot(historyRNN_small)
```

Can we do better by increasing the length of the reviews?

```{r}

# Cut off the text after 100 words (i.e. among the max_features most common words)
maxlen <- 100

# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)
input_train <- pad_sequences(input_train_original, maxlen = maxlen)
input_test <- pad_sequences(input_test_original, maxlen = maxlen)
```

Let's train a simple recurrent network using a `layer_embedding()` and `layer_simple_rnn()`.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

historyRNN_large <- model %>% fit(
  input_train, train_targets,
  epochs = 20,
  batch_size = 128,
  validation_split = 0.15
)
```

```{r}
plot(historyRNN_large)
```
