---
title: "Introduction to Deep Learning"
subtitle: "Classification -- Single-label, Multi-class"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = TRUE)

# Initialize packages
library(keras)
library(tidyverse)
```

# {.tabset .tabset-fade .tabset-pills}

## Functions & Take-home Message

For single-label multi-class classification, we try to predict one of many possible classes for the response variable.

### Take-home Message

In this case study, we'll perform a single-label, multi-class classification to predict one of 4 possible outcomes from 13 predictor variables. 4 is an arbitrary value I've chosen for example purposes, of course in reality there can be many more classes.

We'll use the following functions in our network arguments:

| Name                           | Function                          |
|:-------------------------------|:----------------------------------|
| Loss function                  | `sparse_categorical_crossentropy` |
| Metric                         | `accuracy`                        |
| last-layer activation function | `softmax`                         |

## Part 1: Data Preparation

### Obtain data

```{r}
source("Boston_Z.R")
```

### Prepare the labels:

The target, response variable:

```{r strTargets}
str(train_targets)
str(test_targets)
```

```{r}
# I minus the integers by one so that we begin counting at 0.
train_targets <- as.integer(cut(train_targets, seq(0,50,10))) - 1 
test_targets <- as.integer(cut(test_targets, seq(0,50,10))) - 1

table(train_targets)
table(test_targets)
```

The classes are imbalanced.

```{r echo = FALSE, eval = FALSE}
train_targets %>%
  data.frame(x = .) %>%
  ggplot(aes(x)) +
  geom_histogram(binwidth = 1)
```



## Part 2: Define Network

### Define the network

Here we specify the final activation function. We're going to use the sigmoid activation function, which will return a single value. That matches the format of our labels.

```{r architecture}
network <- keras_model_sequential() %>% 
  layer_dense(units = 2^6, activation = "relu", input_shape = 13) %>% 
  layer_dense(units = 2^6, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")
```

### View a summary of the network

```{r summary}
summary(network)
```

### Compile

```{r compile}
network %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)
```

## Part 3: Train

Now let's train our network for 100 epochs:

```{r echo=TRUE, results = "hide", warning = FALSE}
history <- network %>% fit(
  train_data,
  train_targets,
  epochs = 100,
  batch_size = 16
)
```

Let's display its loss and accuracy curves:

```{r}
plot(history)
```

The network begins to over-fit after about 25 epochs. Let's train a new network from scratch for nine epochs and then evaluate it on the test set.

```{r, echo=TRUE, results='hide'}
network <- keras_model_sequential() %>% 
  layer_dense(units = 2^6, activation = "relu", input_shape = 13) %>% 
  layer_dense(units = 2^6, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")
  
network %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- network %>% fit(
  train_data,
  train_targets,
  epochs = 25,
  batch_size = 16
)
```

## Part 4: Check output

Let's return to our original model using the vectorized data:

### Metrics

```{r metrics}
metrics <- network %>% evaluate(test_data, test_targets)
```

```{r}
metrics
```

The accuracy is `r metrics$acc` and the error rate, i.e. incorrect calling, is `r 1 - metrics$acc`.

### Predictions

```{r predictions}
network %>% predict_classes(test_data[1:10,])
```

```{r allPredictions}
predictions <- network %>% predict_classes(test_data)
actual <- unlist(test_targets)
totalmisses <- sum(predictions != actual)
```

### Confusion Matrix

```{r confusion, echo = F}
suppressPackageStartupMessages(library(tidyverse))
# library(dplyr)
data.frame(target = actual,
           prediction = predictions) %>% 
  filter(target != prediction) %>% 
  group_by(target, prediction) %>%
  count() %>%
  ungroup() %>%
  mutate(perc = n/nrow(.)*100) %>% 
  filter(n > 1) %>% 
  ggplot(aes(target, prediction, size = n)) +
  geom_point(shape = 15, col = "#9F92C6") +
  scale_x_continuous("Actual Target", breaks = 0:4) +
  scale_y_continuous("Prediction", breaks = 0:4) +
  scale_size_area(breaks = c(2,5,10,15), max_size = 5) +
  coord_fixed() +
  ggtitle(paste(totalmisses, "mismatches")) +
  theme_classic() +
  theme(rect = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(colour = "black"))

```

## Vectorization

For softmax, we get the probabilities for each class. For this purpose you'll often see `categorical_crossentropy` as a loss function. To use this, the targets/labels need to be in the same format. We can achieve this using:

```{r}
head(train_targets)
```


```{r}
train_targets_vec <- to_categorical(train_targets)
test_targets_vec <- to_categorical(test_targets)

head(train_targets_vec)
```

