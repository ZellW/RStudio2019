---
title: "Classification and localization"
output:
  html_notebook:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

```{r}
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)

use_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)
```


## Dataset

We'll be using images and annotations from the _Pascal VOC dataset_.
The images are here


```{r}
data_dir <- config::get("data_dir")
img_dir <- file.path(data_dir, "VOCdevkit/VOC2007/JPEGImages")
```

and the annotations are here

```{r}
annot_file <- file.path(data_dir, "pascal_train2007.json")
```


## Preprocessing

Now, the annotations contain information about three types of things we're interested in.

```{r}
annotations <- fromJSON(file = annot_file)
str(annotations, max.level = 1)
```


First, we need the characteristics of the image itself (height and width) and where it's stored. Not surprisingly, here it's one entry per image.

```{r}
imageinfo <- annotations$images %>% {
  tibble(
    id = map_dbl(., "id"),
    file_name = map_chr(., "file_name"),
    image_height = map_dbl(., "height"),
    image_width = map_dbl(., "width")
  )
}
imageinfo
```


Then, object class ids and bounding box coordinates. There may be multiple of these per image.


```{r}
boxinfo <- annotations$annotations %>% {
  tibble(
    image_id = map_dbl(., "image_id"),
    category_id = map_dbl(., "category_id"),
    bbox = map(., "bbox")
  )
}
boxinfo
```

The bounding boxes are now stored in a list column and need to be unpacked.

```{r}
boxinfo <- boxinfo %>% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))
boxinfo <- boxinfo %>% 
  separate(bbox, into = c("x_left", "y_top", "bbox_width", "bbox_height"))
boxinfo <- boxinfo %>% mutate_all(as.numeric)
boxinfo
```

Now we have `x_left` and `y_top` coordinates, as well as width and height.
We will mostly be working with corner coordinates, so we create the missing `x_right` and `y_top`.

As usual in image processing, the `y` axis starts from the top.

```{r}
boxinfo <- boxinfo %>% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)
boxinfo
```


Finally, we still need to match class ids to class names.

```{r}
catinfo <- annotations$categories %>%  {
  tibble(id = map_dbl(., "id"), name = map_chr(., "name"))
}
catinfo
```

Store a shortcut:

```{r}
classes <- catinfo$name
```


So, putting it all together:

```{r}
imageinfo <- imageinfo %>%
  inner_join(boxinfo, by = c("id" = "image_id")) %>%
  inner_join(catinfo, by = c("category_id" = "id"))
imageinfo
```

Note that here still, we have several entries per image, each annotated object occupying its own row.

Soon, we'll run these images through a model that takes a resolution of `224x224`. The bounding box coordinates need to be re-scaled to that size.

```{r}
target_height <- 224
target_width <- 224

imageinfo <- imageinfo %>% mutate(
  x_left_scaled = (x_left / image_width * target_width) %>% round(),
  x_right_scaled = (x_right / image_width * target_width) %>% round(),
  y_top_scaled = (y_top / image_height * target_height) %>% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()
)
imageinfo
```

We're ready. Let's take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields

```{r}
img_data <- imageinfo[4,]
img <- image_read(file.path(img_dir, img_data$file_name))
img <- image_draw(img)
rect(
  img_data$x_left,
  img_data$y_bottom,
  img_data$x_right,
  img_data$y_top,
  border = "white",
  lwd = 2
)
text(
  img_data$x_left,
  img_data$y_top,
  img_data$name,
  offset = 1,
  pos = 2,
  cex = 1.5,
  col = "white"
)
dev.off()
```


## Zooming in on one object


We'll first do classification and localization of a single object, but as this is a multi-object dataset, we need to zoom in on one. A reasonable strategy seems to be choosing the object with the largest ground truth bounding box.

Note: We should be prepared that performance will be worse than had we chosen a single-object dataset, because this dataset _does_ contain multiple salient objects in an image.

```{r}
imageinfo <- imageinfo %>% mutate(area = bbox_width_scaled * bbox_height_scaled)

imageinfo_maxbb <- imageinfo %>%
  group_by(id) %>%
  filter(which.max(area) == row_number())

imageinfo_maxbb
```


## Train-test split

Time for action! Split the dataset (`imageinfo_maxbb`) into 80% training and 20% validation.

```{r}
n_samples <- nrow(imageinfo_maxbb)
train_indices <- sample(1:n_samples, 0.8 * n_samples)
train_data <- imageinfo_maxbb[train_indices,]
validation_data <- imageinfo_maxbb[-train_indices,]
```

We start with classification and then look at localization.

## Single-object classification

### Data streaming

Here's a simple generator that delivers images as well as the corresponding targets in a stream. 
Later we'll adapt this to more complex tasks.

Note how the targets are not one-hot-encoded, but integers - `keras` can handle provided we use the appropriate loss function later.


```{r}
batch_size <- 2

# this preprocessing function will be called from the generator
load_and_preprocess_image <- function(image_name, target_height, target_width) {
  img_array <- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %>%
    image_to_array() %>%
    xception_preprocess_input() 
  dim(img_array) <- c(1, dim(img_array))
  img_array
}

# generator function
# can deliver both shuffled as well as non-shuffled data
# returns images and targets (classes) in a list
classification_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y <- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]],
                                    target_height, target_width)
        y[j, ] <-
          data[[indices[j], "category_id"]] - 1
      }
      x <- x / 255
      list(x, y)
    }
  }

# training generator
train_gen <- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

# validation generator
valid_gen <- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)
```


# Single-object classification model

Here's the model for single-object classification. 

```{r}
feature_extractor <-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = NULL
)

feature_extractor %>% freeze_weights()

model <- keras_model_sequential() %>%
  feature_extractor %>%
  layer_flatten() %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 20, activation = "softmax")
```


```{r}
model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = list("accuracy")
)
```


##### Training and predictions

We will skip actually training the model as this would take too long.
Instead please go on to the first exercise in [2_objectdetection_quizzes.Rmd](2_objectdetection_quizzes.Rmd).

Here, however, is the training code for further reference.


```{r, eval=FALSE}
model %>% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_early_stopping(patience = 2)
  )
)
```

Same as above, here is the code for prediction for further reference.

```{r, eval=FALSE}
model %>% predict(load_and_preprocess_image(
  train_data$file_name[1],
  target_height, target_width)
)
```


### Exercise: Multiple object classification

Now go to [2_objectdetection_quizzes.Rmd](2_objectdetection_quizzes.Rmd) and look at quiz 1 please.


On to working with a single object again, and a new task: localization.

## Single-object localization

### Model

Let's start with the model this time, so it's clear what the generator has to deliver. 

First, code the feature extractor, and freeze its weights. We use Xception, as before.
However, there's an important difference here: We want to keep all spatial information from the before-last layer, so don't do any pooling or flattening.


```{r}
feature_extractor <- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

feature_extractor %>% freeze_weights()
```


Now we append a custom head. At the output layer, you want to have 4 units, one for each bounding box coordinate.

```{r}
model <- keras_model_sequential() %>%
  feature_extractor %>%
  layer_flatten() %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 4)
```


### Metrics

For the cost function, choose a loss common in regression. In addition, we'd like to keep track of (= use as a custom metric) a more tangible quantity: How much do estimate and ground truth overlap?

Overlap is usually measured as _Intersection over Union_, or _Jaccard distance_. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.

Here is an implementation. Use this as an additional metric when compiling the model (hint: wrap it in `custom_metric`):

```{r}
metric_iou <- function(y_true, y_pred) {
  
  # order is [x_left, y_top, x_right, y_bottom]
  intersection_xmin <- k_maximum(y_true[ ,1], y_pred[ ,1])
  intersection_ymin <- k_maximum(y_true[ ,2], y_pred[ ,2])
  intersection_xmax <- k_minimum(y_true[ ,3], y_pred[ ,3])
  intersection_ymax <- k_minimum(y_true[ ,4], y_pred[ ,4])
  
  area_intersection <- (intersection_xmax - intersection_xmin) * 
                       (intersection_ymax - intersection_ymin)
  area_y <- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])
  area_yhat <- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])
  area_union <- area_y + area_yhat - area_intersection
  
  iou <- area_intersection/area_union
  k_mean(iou)
  
}
```

Now compile the model.

```{r}
model %>% compile(
  optimizer = "adam",
  loss = "mae",
  metrics = list(custom_metric("iou", metric_iou))
)
```

### Data generator

Now we have to modify the generator to return bounding box coordinates as targets...

```{r}
localization_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y <- array(0, dim = c(length(indices), 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]], 
                                    target_height, target_width)
        y[j, ] <-
          data[indices[j], c("x_left_scaled",
                             "y_top_scaled",
                             "x_right_scaled",
                             "y_bottom_scaled")] %>% as.matrix()
      }
      x <- x / 255
      list(x, y)
    }
  }

train_gen <- localization_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen <- localization_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)
```

### Train the model

This time, we'll want to train for a short time, say 5 epochs, so we have fun and can draw some bouding boxes. 

Start training now, and let's proceed to the next task /exercise.


```{r}
model %>% fit_generator(
  train_gen,
  epochs = 5,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
   callback_early_stopping(patience = 2)
  )
)
```

### Verify some predictions

Here's a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.

```{r}
plot_image_with_boxes <- function(file_name,
                                  object_class,
                                  box,
                                  scaled = FALSE,
                                  class_pred = NULL,
                                  box_pred = NULL) {
  img <- image_read(file.path(img_dir, file_name))
  if(scaled) img <- image_resize(img, geometry = "224x224!")
  img <- image_draw(img)
  x_left <- box[1]
  y_bottom <- box[2]
  x_right <- box[3]
  y_top <- box[4]
  rect(
    x_left,
    y_bottom,
    x_right,
    y_top,
    border = "cyan",
    lwd = 2.5
  )
  text(
    x_left,
    y_top,
    object_class,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = "cyan"
  )
  if (!is.null(box_pred))
    rect(box_pred[1],
         box_pred[2],
         box_pred[3],
         box_pred[4],
         border = "yellow",
         lwd = 2.5)
  if (!is.null(class_pred))
    text(
      box_pred[1],
      box_pred[2],
      class_pred,
      offset = 0,
      pos = 4,
      cex = 1.5,
      col = "yellow")
  dev.off()
  img %>% image_write(paste0("preds_", file_name))
  plot(img)
}
```

Now look at some predictions from the training set. Given you've not been training the model for so long, don't expect too much. 


```{r}
train_1_8 <- train_data[1:8, c("file_name",
                               "name",
                               "x_left_scaled",
                               "y_top_scaled",
                               "x_right_scaled",
                               "y_bottom_scaled")]

for (i in 1:8) {
  preds <-
    model %>% predict(
      load_and_preprocess_image(train_1_8[i, "file_name"], 
                                target_height, target_width),
      batch_size = 1
  )
  plot_image_with_boxes(train_1_8$file_name[i],
                        train_1_8$name[i],
                        train_1_8[i, 3:6] %>% as.matrix(),
                        scaled = TRUE,
                        box_pred = preds)
}
```

Here are some predictions from a prior training. Looking at the "wrong" predictions, do you have an idea what's going on?

![](images/preds_train.jpg){width=100%}



## Single-object detection (classification + localization)

Now we think how we can obtain class prediction and bounding box at the same time. 

Before we look at the code, please go to the quizzes and look at quiz 2, which  will ask about the basic approach.

### Model for single-object detection

Combining regression and classification into one means we'll want to have two outputs in our model.
We'll thus use the functional API this time. 

Again, we start with XCeption as a feature extractor.

```{r}
feature_extractor <- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)
```

Then, we'll want a common custom head but at the end, two outputs, one for the class prediction and one for the bounding box regression.

```{r}
input <- feature_extractor$input
common <- feature_extractor$output %>%
  layer_flatten(name = "flatten") %>%
  layer_activation_relu() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5)

regression_output <-
  layer_dense(common, units = 4, name = "regression_output")
class_output <- layer_dense(
  common,
  units = 20,
  activation = "softmax",
  name = "class_output"
)

model <- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)
```

Then in `compile`, the losses will have to be a list, like so.
You'd also want to experiment with weighting losses differently.

```{r}
model %>% freeze_weights(to = "flatten")

model %>% compile(
  optimizer = "adam",
  loss = list("mae", "sparse_categorical_crossentropy"),
  #loss_weights = list(
  #  regression_output = 0.05,
  #  class_output = 0.95),
  metrics = list(
    regression_output = custom_metric("iou", metric_iou),
    class_output = "accuracy"
  )
)
```

To be able to train, we'll also need to adapt the data generator. But we'll stop here and move on to (multiple) object detection.