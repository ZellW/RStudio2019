---
title: "Computer Vision"
subtitle: "Optimization"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

# {.tabset .tabset-fade .tabset-pills}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)

# Initialize package
library(keras)
library(tidyverse)
```

## Learning Goals & Functions

### Learning Goals

Access and use pre-trained convnets

### Functions in this session:

Here, we'll use functions specific to obtaining and using pre-trained convnets.

| Function              | Description       |
|:----------------------|:---------------------------------------------------------------------------------------------|
| `application_vgg16()` | Add a pre-trained `vgg16` convnet using weights from imagenet.                               |
| `extract_features()`  | Custom function to extract features using the `vgg16` network form images.                   |
| `array_reshape()`     |  Reshape feature to conform to our network tensor input shape.                               |
| `freeze_weights()`    | Freeze specific weights in a pre-trained model so that they are not updated during training. |

## Data

```{r}
# define the directories:
source("dir_cat_dog.R")
```

## Obtain a pre-trained convnet base

Choose the correct pre-defined convnet and use the `"imagenet weights"`

```{r getConv}
conv_base <- application_vgg16(
  weights = freeze_weights(),
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
```

```{r summaryConv}
conv_base
```

## Extracting features using the pre-trained convolutional base

This will automaticall create training, validation and test sets with the data and labels

```{r dataGen0}

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
  features <- array(0, dim = c(sample_count, 4, 4, 512))
  labels <- array(0, dim = c(sample_count))
  
  generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  
  i <- 0
  
  while(TRUE) {
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)
    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    i <- i + 1
    if (i * batch_size >= sample_count)
      break }
  list(
    features = features,
    labels = labels
  ) }


# Call our functions:
train <- _______(train_dir, 2000)
validation <- _______(validation_dir, 1000)
test <- _______(test_dir, 1000)

```

## Reshape features

Use `array_reshape()` to resize the images.

```{r reshape}
reshape_features <- function(features) {
  _______(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- _______(train$features)
validation$features <- _______(validation$features)
test$features <- _______(test$features)
```

## Define model

Here we only need the densely connected classifier

```{r denseModel}

model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", input_shape = 4 * 4 * 512) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model

```

Compile and train. This will be relatively quick.

```{r modelTrain}

model %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

history1 <- model %>% fit(
  train$features, train$labels,
  epochs = 30,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)

```

Plot results

```{r hist1}
plot(history1)
```

## Combining & Freezing

### Combine layers

Combining a densely-connected neural network with the convolutional base. Begin with initiating your network.

```{r convModel}

model <- _______() %>%
  
  # Add convnet
  _______ %>%
  
  # flatten 
  _______() %>%
  
  # Dense network
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model

```

### Freezing

Make sure to freeze the weights of the convnet.

```{r freezeParam}
cat(length(model$trainable_weights), " trainable weights before freezing.\n")

_______(conv_base)

cat(length(model$trainable_weights), " trainable weights before freezing.\n")

```

## Training

This time we train the model end-to-end with a frozen convolutional base. This will take a bit of time, so I've saved the results for you.

```{r datagen1, eval = FALSE}

train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

test_datagen <-
  image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)


# use the appropriate fit function
history_vgg16 <- model %>% _______(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)

```

Save history and model

```{r eval = FALSE}
save(history_vgg16, file = "history_vgg16.RData")

model %>% save_model_hdf5("cats_and_dogs_vgg16.h5")

```

View history

```{r hist2s}
load("history_vgg16.RData")

plot(history_vgg16)
```

## Evaluation



```{r}
# Execute this in the terminal to download the model:
# wget http://scavetta.academy/models/cats_and_dogs_vgg16_fine.h5

model_vgg16 <- load_model_hdf5("cats_and_dogs_vgg16.h5")

test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

# Use the appropriate evaluate function.
model_vgg16 %>% _______(test_generator, steps = 50)

```


