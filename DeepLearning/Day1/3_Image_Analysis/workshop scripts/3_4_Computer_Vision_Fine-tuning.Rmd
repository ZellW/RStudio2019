---
title: "Computer Vision"
subtitle: "Fine-tuning pre-built model"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

# {.tabset .tabset-fade .tabset-pills}

## Learning Goals & Functions

### Learning Goals

Fine-tune pre-trained convnets.

### Functions in this session:

After freezing large chunks of our convnet, we can choose to unfreeze specific components.

| Function              | Description                               |
|:----------------------|:------------------------------------------|
| `unfreeze_weights()`    | Unfreeze specific weights for training. |

## Data sources

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)

# Initialize package
library(keras)

# define the directories:
source("dir_cat_dog.R")
```

## Pre-trained convnets

Obtain a pre-trained convnet

```{r getConv}
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)


model <- keras_model_sequential() %>%
  conv_base %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model

```

## Freezing

```{r freezeParam}
cat(length(model$trainable_weights), " trainable weights before freezing.\n")

freeze_weights(conv_base)

cat(length(model$trainable_weights), " trainable weights before freezing.\n")

```

```{r summaryConv}
conv_base
```

### Unfreezing

Use the above function to unfreeze weights

```{r}
# Unfreezing previously frozen layers
_______(conv_base, from = "block3_conv1")
```

## Fine-tuning

As previously, I've saved the output for you.

```{r eval = FALSE}
# Fine-tuning the model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = "accuracy"
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 100,
  validation_data = validation_generator,
  validation_steps = 50
)
```

### Save

```{r eval = FALSE}

# save(history, file = "history_vgg16_fine.RData")
# model %>% save_model_hdf5("cats_and_dogs_vgg16_fine.h5")

```

```{r}

# you can download the file here, but the connection may be slow:
# download.file("http://scavetta.academy/models/cats_and_dogs_vgg16_fine.h5", "cats_and_dogs_vgg16_fine.h5")
# or in terminal:
# wget http://scavetta.academy/models/cats_and_dogs_vgg16_fine.h5

# For the workshop it will be provided for you
load("history_vgg16_fine.RData")

plot(history)
```


## Evaluation

```{r}

test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% evaluate_generator(test_generator, steps = 50)

```

We should have stopped training earlier, since the validation loss increases after about 30 epochs. 
