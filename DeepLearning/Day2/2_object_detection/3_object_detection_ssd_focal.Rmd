---
title: "Object detection as in SSD (basic principles)"
output:
  html_notebook:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

```{r}
library(tensorflow)
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)

use_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)
```


We start with the same preprocessing steps as before - but we don't need to zoom in on the most salient object any more.

Here again are the basic preprocessing steps.

# Data loading / preprocessing (same as in previous notebook)

## Dataset

```{r}
data_dir <- config::get("data_dir")
img_dir <- file.path(data_dir, "VOCdevkit/VOC2007/JPEGImages")
```

```{r}
annot_file <- file.path(data_dir, "pascal_train2007.json")
```


## Preprocessing

```{r}
load_and_preprocess_image <- function(image_name, target_height, target_width) {
  img_array <- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %>%
    image_to_array() %>%
    imagenet_preprocess_input() 
  dim(img_array) <- c(1, dim(img_array))
  img_array
}
```


```{r}
annotations <- fromJSON(file = annot_file)
str(annotations, max.level = 1)
```

```{r}
imageinfo <- annotations$images %>% {
  tibble(
    id = map_dbl(., "id"),
    file_name = map_chr(., "file_name"),
    image_height = map_dbl(., "height"),
    image_width = map_dbl(., "width")
  )
}
imageinfo
```

```{r}
boxinfo <- annotations$annotations %>% {
  tibble(
    image_id = map_dbl(., "image_id"),
    category_id = map_dbl(., "category_id"),
    bbox = map(., "bbox")
  )
}
boxinfo
```

```{r}
boxinfo <- boxinfo %>% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))
boxinfo <- boxinfo %>% 
  separate(bbox, into = c("x_left", "y_top", "bbox_width", "bbox_height"))
boxinfo <- boxinfo %>% mutate_all(as.numeric)
boxinfo
```

```{r}
boxinfo <- boxinfo %>% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)
boxinfo
```

```{r}
catinfo <- annotations$categories %>%  {
  tibble(id = map_dbl(., "id"), name = map_chr(., "name"))
}
catinfo
```

```{r}
class_names <- c(catinfo$name, "bg")
n_classes <- 20
```

```{r}
imageinfo <- imageinfo %>%
  inner_join(boxinfo, by = c("id" = "image_id")) %>%
  inner_join(catinfo, by = c("category_id" = "id"))
imageinfo
```

```{r}
target_height <- 224
target_width <- 224

imageinfo <- imageinfo %>% mutate(
  x_left_scaled = (x_left / image_width * target_width) %>% round(),
  x_right_scaled = (x_right / image_width * target_width) %>% round(),
  y_top_scaled = (y_top / image_height * target_height) %>% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()
)
imageinfo
```


# A model for object detection

For reference, let's look at what we did to classify and localize one single object.


```{r, eval=FALSE}
feature_extractor <- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

input <- feature_extractor$input
common <- feature_extractor$output %>%
  layer_flatten(name = "flatten") %>%
  layer_activation_relu() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5)

regression_output <-
  layer_dense(common, units = 4, name = "regression_output")
class_output <- layer_dense(
  common,
  units = 20,
  activation = "softmax",
  name = "class_output"
)

model <- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)
```


Now we start with Resnet instead.

```{r}
feature_extractor <- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)
feature_extractor
```


At this point, we have an output of 7x7x2048.

To arrive at the 4x4 grid cells we want, we apply a convolution with strides 2.

```{r}
input <- feature_extractor$input

common <- feature_extractor$output %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = "same",
    activation = "relu",
    name = "head_conv1_1"
  ) %>%
  layer_batch_normalization() %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = "same",
    activation = "relu",
    name = "head_conv1_2"
  ) %>%
  layer_batch_normalization() %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = "same",
    activation = "relu",
    name = "head_conv1_3"
  ) %>%
  layer_batch_normalization() %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = "same",
    activation = "relu",
    name = "head_conv2"
  ) %>%
  layer_batch_normalization() 

common
```

Now we can do as we did before, attach an output for the bounding boxes and one for the classes.

Note how we don't aggregate over the spatial grid though. Instead, we reshape it so the 4x4 grid cells appear sequentially (`layer_reshape`). 

Here first is the class output. We have 21 classes now (the 20 classes from PASCAL, plus background), and we need to classify each cell.
So we need an output of dimensions 16x21.

```{r}
class_output <-
  layer_conv_2d(
    common,
    filters = 21,
    kernel_size = 3,
    padding = "same",
    name = "class_conv"
  ) %>%
  layer_reshape(target_shape = c(16, 21), name = "class_output")
```

For the bounding box output, we apply a `tanh` activation so that values lie between -1 and 1. This is because they will be used to compute offsets to the 4x4 grid.

We'll skip over the subsequent `layer_lambda` (which does just that) and return to that topic later, when we've covered _anchors_.

```{r, eval=FALSE}
bbox_output <-
  layer_conv_2d(
    common,
    filters = 4,
    kernel_size = 3,
    padding = "same",
    name = "bbox_conv"
  ) %>%
  layer_reshape(target_shape = c(16, 4), name = "bbox_flatten") %>%
  layer_activation("tanh") %>%
  layer_lambda(
    f = function(x) {
      activation_centers <-
        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])
      activation_height_width <-
        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])
      activation_corners <-
        k_concatenate(
          list(
            activation_centers - activation_height_width / 2,
            activation_centers + activation_height_width / 2
          )
        )
     activation_corners
    },
    name = "bbox_output"
  )
```


Now that we have a model, we need to step back and get the data into a form it can work with.

# General preprocessing 

First, let's select just the variables we need for this task. In case you're wondering why we keep the unscaled bounding box coordinates, - that's for plotting only.

```{r}
imageinfo4ssd <- imageinfo %>%
  select(category_id,
         file_name,
         name,
         x_left,
         y_top,
         x_right,
         y_bottom,
         ends_with("scaled"))
```

So far, we have one row per object to be detected, not one row per image as we need. We need to address that first.

```{r}
imageinfo4ssd <- imageinfo4ssd %>%
  group_by(file_name) %>%
  summarise(
    categories = toString(category_id),
    name = toString(name),
    xl = toString(x_left_scaled),
    yt = toString(y_top_scaled),
    xr = toString(x_right_scaled),
    yb = toString(y_bottom_scaled),
    xl_orig = toString(x_left),
    yt_orig = toString(y_top),
    xr_orig = toString(x_right),
    yb_orig = toString(y_bottom),
    cnt = n()
  )

imageinfo4ssd
```


Okay! Let's look at one image:

```{r}
example <- imageinfo4ssd[5, ]
img <- image_read(file.path(img_dir, example$file_name))
name <- (example$name %>% str_split(pattern = ", "))[[1]]
x_left <- (example$xl_orig %>% str_split(pattern = ", "))[[1]]
x_right <- (example$xr_orig %>% str_split(pattern = ", "))[[1]]
y_top <- (example$yt_orig %>% str_split(pattern = ", "))[[1]]
y_bottom <- (example$yb_orig %>% str_split(pattern = ", "))[[1]]
img <- image_draw(img)
for (i in 1:example$cnt) {
  rect(x_left[i],
       y_bottom[i],
       x_right[i],
       y_top[i],
       border = "white",
       lwd = 2)
  text(
    x = as.integer(x_right[i]),
    y = as.integer(y_top[i]),
    labels = name[i],
    offset = 1,
    pos = 2,
    cex = 1,
    col = "white"
  )
}
dev.off()
print(img)

```


# Anchors

Now we construct the anchor boxes. As here we want to have one anchor box per cell, grid cells and anchor boxes, in our case, are the same thing.

Here, first, are the x resp. y coordinates of the centers of the anchor boxes.

```{r}
cells_per_row <- 4
gridsize <- 1/cells_per_row
anchor_offset <- 1 / (cells_per_row * 2) 

anchor_xs <-
  seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>% rep(each = cells_per_row)
anchor_ys <-
  seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>% rep(cells_per_row)
```

Let's plot them.

```{r}
ggplot(data.frame(x = anchor_xs, y = anchor_ys), aes(x, y)) +
  geom_point() +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +
  theme(aspect.ratio = 1)
```



In subsequent manipulations, we sometimes need center coordinates combined with height and width, and sometimes we need the corners (top-left, top-right, bottom-right, bottom-left) of the grid cells (anchor boxes).

We thus compute all of these representations and store them.

Here's the centers + height/width representation:

```{r}
anchor_centers <- cbind(anchor_xs, anchor_ys)
anchor_height_width <- matrix(1 / cells_per_row, nrow = 16, ncol = 2)

anchors <- cbind(anchor_centers, anchor_height_width)
anchors
```

And here's the representation storing corners.

```{r}
hw2corners <- function(centers, height_width) {
  cbind(centers - height_width / 2, centers + height_width / 2) %>% unname()
}

# cells are indicated by (xl, yt, xr, yb)
# successive rows first go down in the image, then to the right
anchor_corners <- hw2corners(anchor_centers, anchor_height_width)
anchor_corners
```


Let's take our sample image again and plot it, this time including the anchor boxes / grid cells.
Note that we display the scaled image now - the way the network is going to see it.

```{r}
example <- imageinfo4ssd[5, ]
name <- (example$name %>% str_split(pattern = ", "))[[1]]
x_left <- (example$xl %>% str_split(pattern = ", "))[[1]]
x_right <- (example$xr %>% str_split(pattern = ", "))[[1]]
y_top <- (example$yt %>% str_split(pattern = ", "))[[1]]
y_bottom <- (example$yb %>% str_split(pattern = ", "))[[1]]


img <- image_read(file.path(img_dir, example$file_name))
img <- image_resize(img, geometry = "224x224!")
img <- image_draw(img)

for (i in 1:example$cnt) {
  rect(x_left[i],
       y_bottom[i],
       x_right[i],
       y_top[i],
       border = "white",
       lwd = 2)
  text(
    x = as.integer(x_right[i]),
    y = as.integer(y_top[i]),
    labels = name[i],
    offset = 0,
    pos = 2,
    cex = 1,
    col = "white"
  )
}
for (i in 1:nrow(anchor_corners)) {
  rect(
    anchor_corners[i, 1] * 224,
    anchor_corners[i, 4] * 224,
    anchor_corners[i, 3] * 224,
    anchor_corners[i, 2] * 224,
    border = "cyan",
    lwd = 1,
    lty = 3
  )
}

dev.off()
print(img)

```


Now it's time to address the possibly greatest mystery when you're new to object detection: How do you actually construct the ground truth input to the network?

That is the so-called "matching problem".

# Matching problem

To train the network, we need to assign the ground truth boxes to the grid cells/anchor boxes.

Assume we've already computed the Jaccard index (= IoU) for all ground truth box - grid cell combinations. We then use the following algorithm:

1. For each ground truth object, find the grid cell it maximally overlaps with.

2. For each grid cell, find the object it overlaps with most.

3. In both cases, identify the _entity_ of greatest overlap as well as the _amount_ of overlap.

4. When criterium (1) applies, it overrides criterium (2).

4. When criterium (1) applies, set the amount overlap to a constant, high value: 1.99.

5. Return the combined result, that is, for each grid cell, the object and amount of best (as per the above criteria) overlap.

```{r}
# overlaps shape is: number of ground truth objects * number of grid cells
map_to_ground_truth <- function(overlaps) {
  
  # for each ground truth object, find maximally overlapping cell (crit. 1)
  # measure of overlap, shape: number of ground truth objects
  prior_overlap <- apply(overlaps, 1, max)
  # which cell is this, for each object
  prior_idx <- apply(overlaps, 1, which.max)
  
  # for each grid cell, what object does it overlap with most (crit. 2)
  # measure of overlap, shape: number of grid cells
  gt_overlap <-  apply(overlaps, 2, max)
  # which object is this, for each cell
  gt_idx <- apply(overlaps, 2, which.max)
  
  # set all definitely overlapping cells to respective object (crit. 1)
  gt_overlap[prior_idx] <- 1.99
  
  # now still set all others to best match by crit. 2
  # actually it's other way round, we start from (2) and overwrite with (1)
  for (i in 1:length(prior_idx)) {
    # iterate over all cells "absolutely assigned"
    p <- prior_idx[i] # get respective grid cell
    gt_idx[p] <- i # assign this cell the object number
  }
  
  # return: for each grid cell, object it overlaps with most + measure of overlap
  list(gt_overlap, gt_idx)
  
}
```

Now here's the IoU calculation we need for that. We can't just use our previous IoU function because this time, we want to compute overlaps with all grid cells simultaneously.
It's easiest to do this using tensors, so we temporarily convert the R matrices to tensors:

```{r}
# compute IOU
jaccard <- function(bbox, anchor_corners) {
  bbox <- k_constant(bbox)
  anchor_corners <- k_constant(anchor_corners)
  intersection <- intersect(bbox, anchor_corners)
  union <-
    k_expand_dims(box_area(bbox), axis = 2)  + k_expand_dims(box_area(anchor_corners), axis = 1) - intersection
    res <- intersection / union
  res %>% k_eval()
}

# compute intersection for IOU
intersect <- function(box1, box2) {
  box1_a <- box1[, 3:4] %>% k_expand_dims(axis = 2)
  box2_a <- box2[, 3:4] %>% k_expand_dims(axis = 1)
  max_xy <- k_minimum(box1_a, box2_a)
  
  box1_b <- box1[, 1:2] %>% k_expand_dims(axis = 2)
  box2_b <- box2[, 1:2] %>% k_expand_dims(axis = 1)
  min_xy <- k_maximum(box1_b, box2_b)
  
  intersection <- k_clip(max_xy - min_xy, min = 0, max = Inf)
  intersection[, , 1] * intersection[, , 2]
  
}

box_area <- function(box) {
  (box[, 3] - box[, 1]) * (box[, 4] - box[, 2]) 
}

```

# Data generator

Now, all this logic has to happen when we create the input and target data,- thus, in the data generator.

The generator has the now familiar structure, but performs some more actions.

We'll talk through them after listing the complete code.

```{r}
batch_size <- 16
image_size <- target_width # same as height

threshold <- 0.4

class_background <- 21

ssd_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y1 <- array(0, dim = c(length(indices), 16))
      y2 <- array(0, dim = c(length(indices), 16, 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]], target_height, target_width)
        
        class_string <- data[indices[j], ]$categories
        xl_string <- data[indices[j], ]$xl
        yt_string <- data[indices[j], ]$yt
        xr_string <- data[indices[j], ]$xr
        yb_string <- data[indices[j], ]$yb
        
        classes <-  str_split(class_string, pattern = ", ")[[1]]
        xl <-
          str_split(xl_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
        yt <-
          str_split(yt_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
        xr <-
          str_split(xr_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
        yb <-
          str_split(yb_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
    
        # rows are objects, columns are coordinates (xl, yt, xr, yb)
        # anchor_corners are 16 rows with corresponding coordinates
        bbox <- cbind(xl, yt, xr, yb)
        overlaps <- jaccard(bbox, anchor_corners)
        
        c(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)
        gt_class <- classes[gt_idx]
        
        pos <- gt_overlap > threshold
        gt_class[gt_overlap < threshold] <- 21
                
        # columns correspond to objects
        boxes <- rbind(xl, yt, xr, yb)
        # columns correspond to object boxes according to gt_idx
        gt_bbox <- boxes[, gt_idx]
        # set those with non-sufficient overlap to 0
        gt_bbox[, !pos] <- 0
        gt_bbox <- gt_bbox %>% t()
        
        y1[j, ] <- as.integer(gt_class) - 1
        y2[j, , ] <- gt_bbox
        
      }

      x <- x %>% imagenet_preprocess_input()
      y1 <- y1 %>% to_categorical(num_classes = class_background)
      list(x, list(y1, y2))
    }
  }

```

Before the generator can trigger any calculations, it needs to first split apart the multiple classes and bounding box coordinates. 

To make this more concrete, we show the results for the "2 people and 2 airplanes" image we just displayed.

We copy out code step-by-step from the generator so results can actually be displayed for inspection.

```{r}
data <- imageinfo4ssd
indices <- 1:8

j <- 5 # this is our image

class_string <- data[indices[j], ]$categories
xl_string <- data[indices[j], ]$xl
yt_string <- data[indices[j], ]$yt
xr_string <- data[indices[j], ]$xr
yb_string <- data[indices[j], ]$yb
        
classes <-  str_split(class_string, pattern = ", ")[[1]]
xl <- str_split(xl_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
yt <- str_split(yt_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
xr <- str_split(xr_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
yb <- str_split(yb_string, pattern = ", ")[[1]] %>% as.double() %>% `/`(image_size)
```

So here are that image's `classes`:

```{r}
classes
```

And its left bounding box coordinates:

```{r}
xl
```

Now we can `cbind` those vectors to obtain a object (`bbox`) where rows are objects, and coordinates are in the columns: 

```{r}
# rows are objects, columns are coordinates (xl, yt, xr, yb)
bbox <- cbind(xl, yt, xr, yb)
bbox
```

So we're ready to compute these boxes' overlap with all of the 16 grid cells. Recall that `anchor_corners` stores the grid cells in an analogous way, the cells being in the rows and the coordinates in the columns.

```{r}
# anchor_corners are 16 rows with corresponding coordinates
overlaps <- jaccard(bbox, anchor_corners)
overlaps
```


Now that we have the overlaps, we can call the matching logic:

```{r}
c(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)
gt_overlap
```


Looking for the value `1.99` in the above - the value indicating maximal, by the above criteria, overlap of an object with a grid cell - we see that boxes 4 (counting in column-major order here like R does) got matched (to a person, as we'll see soon), the sixth did (to an airplane), and the seventh did (to a person). How about the other airplane? It got lost in the matching.

This is not a problem of the matching algorithm though - it would disappear if we had more than one anchor box per grid cell.

Looking for the objects we just mentioned in the class index, `gt_idx`, we see that indeed box 4 got matched to object 4 (a person), box 6 got matched to object 2 (an airplane), and box 7 got matched to object 3 (the other person):

```{r}
gt_idx
```

By the way, don't worry about the abundance of `1`s here. These are remnants from using `which.max` to determine maximal overlap, and will disappear soon.

Instead of thinking in object numbers, we should think in object classes (the numerical codes that is).

```{r}
gt_class <- classes[gt_idx]
gt_class
```

So far, we count as overlap even the very slightest overlap - of 0.1 percent even, say.
Of course, this makes no sense. We set all cells with an overlap < 0.4 to the background class:

```{r}
 pos <- gt_overlap > threshold
gt_class[gt_overlap < threshold] <- 21

gt_class
```

Now, to construct the targets for learning, we need to put the bounding boxes for the objects that were found present in a cell into a matrix matching boxes to cells.

The following gives us a 16x4 matrix of cells and the boxes they are responsible for:

```{r}
orig_boxes <- rbind(xl, yt, xr, yb)
# columns correspond to object boxes according to gt_idx
gt_bbox <- orig_boxes[, gt_idx]
# set those with non-sufficient overlap to 0
gt_bbox[, !pos] <- 0
gt_bbox <- gt_bbox %>% t()

gt_bbox
```

It is `gt_bbox`, together with `gt_class`, that we'll return as targets for the net.

```{r, eval=FALSE}
y1[j, ] <- as.integer(gt_class) - 1
y2[j, , ] <- gt_bbox
```

To summarize, our target is a list of two outputs, the bounding box ground truth of dimensions _number of grid cells_ times _number of box coordinates (4)_, and the class ground truth of size _number of grid cells_ times _number of classes_.

We can verify this by asking the generator for a batch of inputs and targets:

```{r}
train_gen <- ssd_generator(
  imageinfo4ssd,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

batch <- train_gen()
c(x, c(y1, y2)) %<-% batch
dim(y1)
```

```{r}
dim(y2)
```



# A second look at the model

Now that we've seen the targets, let's take a second look at the bounding box output of the model.
Here it is again.

In the `layer_lambda`, we start from the actual anchor box centers, and move them around by a scaled-down version of the activations.
We then convert these to anchor corners (same as we did above with the ground truth anchors, just operating on tensors this time).


```{r, eval=TRUE}
bbox_output <-
  layer_conv_2d(
    common,
    filters = 4,
    kernel_size = 3,
    padding = "same",
    name = "bbox_conv"
  ) %>%
  layer_permute(dims = c(2,1,3)) %>%
  layer_reshape(target_shape = c(16, 4), name = "bbox_flatten") %>%
  layer_activation("tanh") %>%
  layer_lambda(
    f = function(x) {
      activation_centers <-
        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])
      activation_height_width <-
        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])
      activation_corners <-
        k_concatenate(
          list(
            activation_centers - activation_height_width / 2,
            activation_centers + activation_height_width / 2
          )
        )
     activation_corners
    },
    name = "bbox_output"
  )

```

Let's quickly finish up the model definition:


```{r}

model <-
  keras_model(inputs = input,
              outputs = list(class_output, bbox_output))
```

The last ingredient missing is the loss function.

# Loss 

To the model's two outputs - a classification output and a regression output - correspond two losses, just as in the basic classification + localization model. Only this time, we have 16 grid cells to take care of.

The class loss uses `tf$nn$sigmoid_cross_entropy_with_logits` to compute the binary crossentropy between targets and unnormalized network activation, summing over grid cells and dividing by the number of classes.

```{r}
# shapes are batch_size * 16 * 21
class_loss <- function(y_true, y_pred) {
  # leave out background for class loss calculation
  #class_loss  <-
  #  tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true[, , 1:20], logits = y_pred[, , 1:20])
  class_loss  <-
    tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)

  class_loss <-
    tf$reduce_sum(class_loss) / tf$cast(n_classes + 1, "float32")
  
  class_loss
}

```

The localization loss is calculated for all boxes where in fact there is an object present in the ground truth. For that, we calculate a mask and use that to mask out all activations that are irrelevant to the task.

The loss itself then is mean absolute error, scaled by a multiplier designed to bring both loss components to similar magnitudes.

```{r}
# shapes are batch_size * 16 * 4
bbox_loss <- function(y_true, y_pred) {
  # calculate localization loss for all boxes where ground truth was assigned some overlap
  # calculate mask
  pos <- y_true[, , 1] + y_true[, , 3] > 0
  pos <-
    pos %>% k_cast(tf$float32) %>% k_reshape(shape = c(batch_size, 16, 1))
  pos <-
    tf$tile(pos, multiples = k_constant(c(1L, 1L, 4L), dtype = tf$int32))
  diff <- y_pred - y_true
  # mask out irrelevant activations
  diff <- diff %>% tf$multiply(pos)
  loc_loss <- diff %>% tf$abs() %>% tf$reduce_mean()
  
  loc_loss * 100
}

```


# Training


Above, we've already defined the model but we still need to freeze the feature detector's weights and compile it.

```{r}
model %>% freeze_weights()
model %>% unfreeze_weights(from = "head_conv1")
model
```

```{r}
model %>% compile(
  loss = list(class_loss, bbox_loss),
  optimizer = "adam",
  metrics = list(
    class_output = custom_metric("class_loss", metric_fn = class_loss),
    bbox_output = custom_metric("bbox_loss", metric_fn = bbox_loss)
  )
)
```


And we're ready to train.

```{r}
steps_per_epoch <- nrow(imageinfo4ssd) / batch_size

#model %>% fit_generator(
#   train_gen,
#  steps_per_epoch = steps_per_epoch,
#  epochs = 5,
#  callbacks = callback_model_checkpoint("weights_withbg.{epoch:02d}-{loss:.2f}.hdf5", 
#                                        save_weights_only = TRUE)
#)

```



# Focal loss

```{r}
alpha <- 0.25
gamma <- 1


get_weights <- function(y_true, y_pred) {
  p <- y_pred %>% k_sigmoid()
  pt <-  y_true*p + (1-p)*(1-y_true)
  w <- alpha*y_true + (1-alpha)*(1-y_true)
  w <-  w * (1-pt)^gamma
  w
}

class_loss_focal  <- function(y_true, y_pred) {
  
  w <- get_weights(y_true, y_pred)
  cx <- tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)
  weighted_cx <- w * cx

  class_loss <-
   tf$reduce_sum(weighted_cx) / tf$cast(21, "float32")
  
  class_loss
}
```


```{r}
feature_extractor <- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)
input <- feature_extractor$input

common <- feature_extractor$output %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = "same",
    name = "head_conv1"
  ) %>%
  layer_batch_normalization() %>%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = "same",
    name = "head_conv2"
  ) %>%
  layer_batch_normalization() 

class_output <-
  layer_conv_2d(
    common,
    filters = 21,
    kernel_size = 3,
    padding = "same",
    name = "class_conv"
  ) %>%
  layer_reshape(target_shape = c(16, 21), name = "class_output")

bbox_output <-
  layer_conv_2d(
    common,
    filters = 4,
    kernel_size = 3,
    padding = "same",
    name = "bbox_conv"
  ) %>%
  layer_permute(dims = c(2,1,3)) %>%
  layer_reshape(target_shape = c(16, 4), name = "bbox_flatten") %>%
  layer_activation("tanh") %>%
  layer_lambda(
    f = function(x) {
      activation_centers <-
        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])
      activation_height_width <-
        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])
      activation_corners <-
        k_concatenate(
          list(
            activation_centers - activation_height_width / 2,
            activation_centers + activation_height_width / 2
          )
        )
     activation_corners
    },
    name = "bbox_output"
  )

model <-
  keras_model(inputs = input,
              outputs = list(class_output, bbox_output))

model %>% freeze_weights()
model %>% unfreeze_weights(from = "head_conv1")

model %>% compile(
  loss = list(class_loss_focal, bbox_loss),
  optimizer = "adam",
  metrics = list(
    class_output = custom_metric("class_loss_focal", metric_fn = class_loss_focal),
    bbox_output = custom_metric("bbox_loss", metric_fn = bbox_loss)
  )
)
```


```{r}
steps_per_epoch <- nrow(imageinfo4ssd) / batch_size
#model %>% fit_generator(
#  train_gen,
#  steps_per_epoch = steps_per_epoch,
#  epochs = 5,
#  callbacks = callback_model_checkpoint("weights_focal.{epoch:02d}-{loss:.2f}.hdf5", #save_weights_only = TRUE)
#)
```


